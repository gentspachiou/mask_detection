{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LBZ9VWZZFUCT"
      },
      "outputs": [],
      "source": [
        "#installing tensorflow version 2.2.0 in Colab\n",
        "!pip install -U --pre tensorflow==\"2.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "outputs": [],
      "source": [
        "# Install tensorflows Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZcqD4NLdnf4"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import imageio\n",
        "import glob\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "\n",
        "  Args:\n",
        "    path: a file path.\n",
        "\n",
        "  Returns:\n",
        "    numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  \n",
        "  (im_width, im_height) = image.size\n",
        "  #print(image.size)\n",
        " \n",
        "  #removing alpha channel from png images\n",
        "  img_array = np.array(image.getdata())[:,:3]\n",
        "  return img_array.reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "  \"\"\"Wrapper function to visualize detection boxes.\n",
        "\n",
        "  Args:\n",
        "    image_np: numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "      and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "      this function assumes that the boxes to be plotted are groundtruth\n",
        "      boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "      category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "  \"\"\"\n",
        "  #check if grountruth boxes is empty\n",
        "  if boxes is not None:\n",
        "    #a copy of the image with annotations\n",
        "    image_np_with_annotations = image_np.copy()\n",
        "    #function from Object Detection API create visualization of dection boxes and\n",
        "    #classes\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_annotations,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        #threshhold for detecting if score is bigger\n",
        "        min_score_thresh=0.5)\n",
        "    if image_name:\n",
        "      plt.imsave(image_name, image_np_with_annotations)\n",
        "    else:\n",
        "      plt.imshow(image_np_with_annotations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaXL28TZfk1"
      },
      "source": [
        "# Face masks data\n",
        "\n",
        "We will start with limited amount of data just consisting of 5 images of a images with face masks.  The  [coco](https://cocodataset.org/#explore) dataset contains 91 classes but not a specific class for people's faces with masks (contains a class person tough), so this is a novel class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb55u55f_WN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e98d12-9cb1-4dfa-8e2e-630a3f99ccf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6wG0kTWAUN_"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/MyDrive/data.zip\"\n",
        "with zipfile.ZipFile(data_dir , 'r') as zip_ref:\n",
        "  zip_ref.extractall( '/content/'  )  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQy3ND7EpFQM"
      },
      "outputs": [],
      "source": [
        "# Load images and visualize\n",
        "train_image_dir = '/content/imagesFaceMask'\n",
        "train_images_np = []\n",
        "#selecting only 10 images from the entire dataset\n",
        "for i in range(0, 25):\n",
        "  image_path = os.path.join(train_image_dir, 'maksssksksss' + str(i) + '.png')\n",
        "  #print(\"image_path: \", image_path)\n",
        "  train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "  plt.subplot(6, 5, idx+1)\n",
        "  plt.imshow(train_image_np)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbKXmQoxcUgE"
      },
      "source": [
        "# Annotate images with bounding boxes\n",
        "\n",
        "In this cell I will annotate the face mask mages --- draw a box around  in each image.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nEDRoUEcUgL"
      },
      "outputs": [],
      "source": [
        "gt_boxes = []\n",
        "#colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H94L0qNUv0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f993155-7df6-477c-db38-6d6495fdf46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(gt_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6wVh_bvBrAB"
      },
      "outputs": [],
      "source": [
        "ground_truths = gt_boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZUhr33n-Moy"
      },
      "outputs": [],
      "source": [
        "#memorize the groundtruth coordinates so whenever starting anew no need to annotate again from scratch\n",
        "gt_boxes = [np.array([[0.28940104, 0.34960938, 0.40273438, 0.44726562]]), np.array([[0.30273438, 0.55      , 0.47273438, 0.645     ],\n",
        "       [0.29606771, 0.7875    , 0.43440104, 0.87      ],\n",
        "       [0.50940104, 0.0375    , 0.58773438, 0.105     ]]), np.array([[0.17773438, 0.175     , 0.23940104, 0.25      ],\n",
        "       [0.18606771, 0.3725    , 0.25606771, 0.435     ],\n",
        "       [0.15940104, 0.585     , 0.23773437, 0.665     ],\n",
        "       [0.16106771, 0.8075    , 0.23106771, 0.9225    ]]), np.array([[0.21108333, 0.12      , 0.27275   , 0.1775    ],\n",
        "       [0.21275   , 0.18      , 0.26275   , 0.2225    ],\n",
        "       [0.23608333, 0.385     , 0.29275   , 0.4375    ],\n",
        "       [0.24441667, 0.4625    , 0.28608333, 0.5375    ],\n",
        "       [0.22775   , 0.5775    , 0.28441667, 0.6475    ],\n",
        "       [0.23608333, 0.71      , 0.29441667, 0.7725    ],\n",
        "       [0.26608333, 0.78      , 0.32275   , 0.8275    ]]), np.array([[0.54275   , 0.2192691 , 0.79941667, 0.59136213]]), np.array([[0.40608333, 0.595     , 0.51275   , 0.745     ]]), np.array([[0.33608333, 0.36333333, 0.63608333, 0.64666667]]), np.array([[0.50275   , 0.31      , 0.64108333, 0.4325    ],\n",
        "       [0.40608333, 0.38      , 0.58108333, 0.5575    ],\n",
        "       [0.46108333, 0.61      , 0.69275   , 0.8525    ]]), np.array([[0.70608333, 0.2192691 , 0.95275   , 0.49169435]]), np.array([[0.22941667, 0.54307116, 0.33775   , 0.72284644]]), np.array([[0.72608333, 0.30232558, 0.94941667, 0.64451827]]), np.array([[0.50108333, 0.0325    , 0.64108333, 0.1675    ],\n",
        "       [0.64941667, 0.3575    , 0.78775   , 0.4925    ],\n",
        "       [0.77275   , 0.5925    , 0.92608333, 0.725     ],\n",
        "       [0.72941667, 0.76      , 0.84108333, 0.8675    ],\n",
        "       [0.39441667, 0.6775    , 0.47275   , 0.7525    ],\n",
        "       [0.42608333, 0.6175    , 0.51275   , 0.6875    ],\n",
        "       [0.30608333, 0.5325    , 0.35775   , 0.58      ],\n",
        "       [0.35941667, 0.485     , 0.41775   , 0.535     ],\n",
        "       [0.41441667, 0.445     , 0.46608333, 0.5075    ],\n",
        "       [0.11608333, 0.7175    , 0.14608333, 0.7575    ],\n",
        "       [0.19941667, 0.57      , 0.24608333, 0.6175    ],\n",
        "       [0.18941667, 0.51      , 0.23275   , 0.5525    ]]), np.array([[0.29941667, 0.405     , 0.36108333, 0.4575    ],\n",
        "       [0.27108333, 0.29      , 0.32108333, 0.3425    ],\n",
        "       [0.27441667, 0.1775    , 0.32275   , 0.2275    ],\n",
        "       [0.23775   , 0.5775    , 0.28608333, 0.6175    ],\n",
        "       [0.22775   , 0.6775    , 0.27441667, 0.7275    ],\n",
        "       [0.26941667, 0.8275    , 0.32275   , 0.8875    ],\n",
        "       [0.21441667, 0.295     , 0.24275   , 0.3275    ]]), np.array([[0.29775   , 0.3975    , 0.39108333, 0.49      ],\n",
        "       [0.28275   , 0.725     , 0.34941667, 0.78      ],\n",
        "       [0.25608333, 0.875     , 0.31275   , 0.9175    ]]), np.array([[0   , 0    , 0, 0      ]]), np.array([[0.84108333, 0.40932642, 0.91775   , 0.60621762]]), np.array([[0.63108333, 0.11627907, 0.83275   , 0.48172757]]), np.array([[0.50275   , 0.22259136, 0.72941667, 0.57475083]]), np.array([[0.38608333, 0.32      , 0.54941667, 0.4825    ],\n",
        "       [0.65775   , 0.7525    , 0.79608333, 0.885     ]]), np.array([[0.54608333, 0.255     , 0.73941667, 0.4375    ],\n",
        "       [0.41441667, 0.7725    , 0.57441667, 0.9275    ]]), np.array([[0.18941667, 0.33707865, 0.30941667, 0.57677903]]), np.array([[0.68108333, 0.095     , 0.89441667, 0.3       ],\n",
        "       [0.40775   , 0.465     , 0.61108333, 0.6075    ],\n",
        "       [0.24941667, 0.775     , 0.42775   , 0.885     ],\n",
        "       [0.31941667, 0.1275    , 0.47941667, 0.27      ]]), np.array([[0.45941667, 0.445     , 0.68775   , 0.6325    ]]), np.array([[0.28775   , 0.47      , 0.43941667, 0.5975    ]]), np.array([[0.28941667, 0.59550562, 0.29108333, 0.59550562],\n",
        "       [0.22441667, 0.35955056, 0.29275   , 0.61048689]])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb_yjAo3cO_"
      },
      "source": [
        "# Prepare data for training\n",
        "\n",
        "Below I add the class annotations, I have only one class to detect here which is the mask presence class.  Everything is converted to the format that the training\n",
        "loop below expects (e.g., everything converted to tensors, classes converted to one-hot representations, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWBqFVMcweF-",
        "outputId": "dc55d078-d946-4ece-eca4-5fc6bed916d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done prepping data.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# By convention, our non-background classes start counting at 1.  Given\n",
        "# that we will be predicting just one class, we will therefore assign it a\n",
        "# `class id` of 1.\n",
        "mask = 1\n",
        "num_classes = 1\n",
        "\n",
        "#the class id and name dictionary holding the classes present\n",
        "category_index = {mask: {'id': mask, 'name': 'mask'}}\n",
        "\n",
        "# Convert class labels to one-hot; convert everything to tensors.\n",
        "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
        "# we do this here so that the model receives one-hot labels where non-background\n",
        "# classes start counting at the zeroth index.\n",
        "\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "#lists to hold one hot tensors of groundtruth boxes and classes\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "\n",
        "for (train_image_np, gt_box_np) in zip(\n",
        "    train_images_np, gt_boxes):\n",
        "  #in case the image is not present or there are no groundtruth boxes \n",
        "  if train_image_np is not None and gt_box_np is not None:\n",
        "    #expand dimensions and convert to tensor to make ready for model\n",
        "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "        train_image_np, dtype=tf.float32), axis=0))\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "\n",
        "    #shift classes by id offset\n",
        "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "    #convert ot one hot tensors and append to classes list\n",
        "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "        zero_indexed_groundtruth_classes, num_classes))\n",
        "    \n",
        "print('Done prepping data.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_Z3mJWN9KJ"
      },
      "source": [
        "# Let's visualize the masks from the annotated ground truth classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBD6l-E4N71y"
      },
      "outputs": [],
      "source": [
        "#give all annotation a 100% detection score for plot visualization\n",
        "dummy_scores = np.array([1.0]*25, dtype=np.float32)  # give boxes a score of 100%\n",
        "\n",
        "plt.figure(figsize=(30, 15))\n",
        "#choose the 25 first annotated images\n",
        "for idx in range(25):\n",
        "  plt.subplot(6, 5, idx+1)\n",
        "  if gt_boxes[idx] is not None:\n",
        "    #run plot_detections above\n",
        "    plot_detections(\n",
        "        train_images_np[idx],\n",
        "        gt_boxes[idx],\n",
        "        np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "        dummy_scores, category_index)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghDAsqfoZvPh"
      },
      "source": [
        "# Create model and restore weights for all but last layer\n",
        "\n",
        "In this cell I build a single stage detection architecture (RetinaNet) and restore all but the classification layer at the top (which will be automatically randomly initialized).\n",
        "\n",
        "For simplicity, a number of things in this colab have been preset for the specific RetinaNet architecture at hand (including assuming that the image size will always be 640x640)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J16r3NChD-7"
      },
      "outputs": [],
      "source": [
        "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
        "\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyT4BUbaMeG-",
        "outputId": "81792f54-9a3a-4429-e56e-ed0f01f1b76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model and restoring weights for fine-tuning...\n",
            "Weights restored!\n"
          ]
        }
      ],
      "source": [
        "#clear session for restarting\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
        "num_classes = 1\n",
        "#the path of the model\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "#the path where the weights are stored\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# Load pipeline config and build a detection model.\n",
        "#\n",
        "# I am using the COCO architecture which in default predicts 90\n",
        "# class slots, override the `num_classes` field here to be just\n",
        "# one. The COCO model was trained with 118K images and brings forth many featers \n",
        "#learned that are  very useful for our few shot detection training\n",
        "\n",
        "#configur the pipeline configurations\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "#chnage the numebr of classes\n",
        "model_config.ssd.num_classes = num_classes\n",
        "#freze batch normalization within the model\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "#build the model wiht the settings above\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)\n",
        "\n",
        "# Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
        "# `heads` --- one for classification, the other for box regression.  I will\n",
        "# restore the box regression head used for the bounding box but initialize the \n",
        "#classification head from scratch because we have an entirely new class to predict\n",
        "#(we show the omission below by commenting out the line that we would add if \n",
        "#we wanted to restore both heads)\n",
        "\n",
        "\n",
        "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
        "    #restore bounding box regression head\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # initialize the classification head from scratch\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "\n",
        "fake_model = tf.compat.v2.train.Checkpoint(\n",
        "          #frozen feature extracting point from model\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          #our final defined layers fro bounding box and class\n",
        "          _box_predictor=fake_box_predictor)\n",
        "#checkpoint from where the model will pick up further training \n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
        "#restoration of checkpoint\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "# Run model through a dummy image so that variables are created it is necessary to do so\n",
        "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "prediction_dict = detection_model.predict(image, shapes)\n",
        "#the model has a postprocess method which creates a prediction dictionary\n",
        "#as an output from which information about predictions are to be extracted\n",
        "_ = detection_model.postprocess(prediction_dict, shapes)\n",
        "print('Weights restored!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh4cygOhM6VX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inspect the layers of detection_model\n",
        "for i,v in enumerate(detection_model.trainable_variables):\n",
        "    print(f\"i: {i} \\t name: {v.name} \\t shape:{v.shape} \\t dtype={v.dtype}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2MBBCpANHVS"
      },
      "source": [
        "As we can see from above our pretrained detection model has 268 layers in total (namely convolutions, bathnormalizations, fully connected) layers. It is a state of the art detection model that has high performance in detection of many objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCkWmdoZZ0zJ"
      },
      "source": [
        "##Custom training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyHoF4mUrv5-",
        "outputId": "503b6c27-fe3b-45fc-90be-841c0e61153f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start fine-tuning!\n",
            "batch 0 of 100, loss=1.7639313\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function get_model_train_step_function.<locals>.train_step_fn at 0x7f2f5b8360e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function get_model_train_step_function.<locals>.train_step_fn at 0x7f2f5b8360e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "batch 10 of 100, loss=1.0625103\n",
            "batch 20 of 100, loss=0.80171895\n",
            "batch 30 of 100, loss=0.67687976\n",
            "batch 40 of 100, loss=0.5348338\n",
            "batch 50 of 100, loss=0.52442133\n",
            "batch 60 of 100, loss=0.5032982\n",
            "batch 70 of 100, loss=0.3376547\n",
            "batch 80 of 100, loss=0.3898978\n",
            "batch 90 of 100, loss=0.3391522\n",
            "Done fine-tuning!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#set the training of the model to True to indicate training phase\n",
        "detection_model.trainable = True\n",
        "\n",
        "# These parameters can be tuned; since our training set has 25 images\n",
        "# it doesn't make sense to have a much larger batch size, though we could\n",
        "# fit more examples in memory if we wanted to, set the learning rate and number \n",
        "#of batches to 100 (could possibly train longer)\n",
        "batch_size = 19\n",
        "learning_rate = 0.01\n",
        "num_batches = 100\n",
        "\n",
        "# Select variables in top layers to fine-tune.\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "#these are the names of the 2 top layers (for bounding box regression and class \n",
        "#classification) that we need to train further, the rest of the layers is frozen\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "\n",
        "for var in trainable_variables:\n",
        "  #add in in fine tuning only the above layers\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "# Set up forward + backward pass for a single train step.\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "  \"\"\"Get a tf.function for training step. This will convert to tensorflow graph \n",
        "  and will increase the running speed\"\"\"\n",
        "\n",
        "  @tf.function\n",
        "  def train_step_fn(image_tensors,\n",
        "                    groundtruth_boxes_list,\n",
        "                    groundtruth_classes_list):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 640x640.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "    shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
        "    #method of model that provides the ground truths\n",
        "    model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "    #starting gradient tape method to compute changes in trainable variables\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      preprocessed_images = tf.concat(\n",
        "          #callin the preprocess method of the detection model to make images suitable \n",
        "          #for the model\n",
        "          [detection_model.preprocess(image_tensor)[0]\n",
        "           for image_tensor in image_tensors], axis=0)\n",
        "      \n",
        "      #the prediction dict output of the model predictions\n",
        "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
        "\n",
        "      #this model outputs losses in a dictionary \n",
        "      losses_dict = model.loss(prediction_dict, shapes)\n",
        "\n",
        "      #we have to sum up for the total loss from both top layers namely the classification \n",
        "      #and bounding box regression layer\n",
        "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "\n",
        "      #calcuate the gradients from the loss with respect to trainable variables\n",
        "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "      \n",
        "      #update the trainable variables(weights)\n",
        "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "  return train_step_fn\n",
        "\n",
        "#the gradient optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "#initialize the step training function above to the following object\n",
        "train_step_fn = get_model_train_step_function(\n",
        "    detection_model, optimizer, to_fine_tune)\n",
        "\n",
        "print('Start fine-tuning!', flush=True)\n",
        "#run for all batches set\n",
        "for idx in range(num_batches):\n",
        "  # Grab keys for a random subset of examples\n",
        "  all_keys = list(range(len(train_images_np)))\n",
        "  random.shuffle(all_keys)\n",
        "  example_keys = all_keys[:batch_size]\n",
        "\n",
        "  # Note that we do not do data augmentation in this demo.  If you want a\n",
        "  # a fun exercise, we recommend experimenting with random horizontal flipping\n",
        "  # and random cropping\n",
        "\n",
        "  #get the ground truth boxes list as tensors\n",
        "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "\n",
        "  #get the ground truth classes list as tensors\n",
        "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "  \n",
        "  #get the images list as tensors\n",
        "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "  # Training step (forward pass + backwards pass) as defined above\n",
        "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "\n",
        "  if idx % 10 == 0:\n",
        "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('Done fine-tuning!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHlXL1x_Z3tc"
      },
      "source": [
        "# Let's Test some images on the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcE6OwrHQJya"
      },
      "outputs": [],
      "source": [
        "test_image_dir = '/content/imagesFaceMask'\n",
        "test_images_np = []\n",
        "#get the images from the images directory in this numerical range\n",
        "for i in range(50, 75):\n",
        "  image_path = os.path.join(test_image_dir, 'maksssksksss' + str(i) + '.png')\n",
        "  #print(image_path)\n",
        "  #convert into numpy and add to list\n",
        "  test_images_np.append(np.expand_dims(\n",
        "      load_image_into_numpy_array(image_path), axis=0))\n",
        "\n",
        "# Lets use tensorflow graph for speed\n",
        "#@tf.function\n",
        "def detect(input_tensor):\n",
        "  \"\"\"Run detection on an input image.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      The height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`) the outputs of our model\n",
        "  \"\"\"\n",
        "  #preprocess the images as per model method\n",
        "  preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "  #get the prediciton dict form the model\n",
        "  prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "\n",
        "  return detection_model.postprocess(prediction_dict, shapes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accounting for background class\n",
        "label_id_offset = 1\n",
        "\n",
        "#number of images tested\n",
        "n = len(test_images_np)\n",
        "\n",
        "\n",
        "#loop through the images\n",
        "for i in range(n):\n",
        "  \n",
        "  #convert image to tensor as model expects\n",
        "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "  #predict the image\n",
        "  detections = detect(input_tensor)\n",
        "  \n",
        "\n",
        "  #run plot_detections above\n",
        "  plot_detections(\n",
        "    test_images_np[i][0],\n",
        "    detections['detection_boxes'][0].numpy(),\n",
        "    detections['detection_classes'][0].numpy().astype(np.uint32)\n",
        "    + label_id_offset,\n",
        "    detections['detection_scores'][0].numpy(),\n",
        "    category_index, figsize=(30, 16))\n",
        "  plt.show()\n",
        "   #\"gif_frame_\" + ('%02d' % i) + \".jpg\")"
      ],
      "metadata": {
        "id": "8Fz-Gt37vjOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the model has learned useful features and detects the presence of masks in mamy cases (also dependent in the detection threshold) but sometimes it fails detecting masks. This is as a result of two factors:\n",
        "1. The image quality and size differs from image to image as well as background, lighting, hue and so on..which makes the learning of features much diffictult. Basically even the images are not from a standardized distribution especially with those that the pretrained COCO model has been trained.\n",
        "\n",
        "2. We need to increase the number of training images fed into the model during the training phase.\n",
        "\n",
        "The model has good predictions for those cases where similar structural images are used, which means the feature detection with a pretrained model is doing a good job but needs to be extended to more images or the images dataset needs to be harmonized and cleaned to a certain standard."
      ],
      "metadata": {
        "id": "XXzHxgcnw_ve"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYF23Q3nVaQy"
      },
      "source": [
        "## Automation of input images from XML annotations\n",
        "\n",
        "Initially I tried to automate the process of reading annotations directly from the XML files of the images throug the following script  but for some reason the coordinate annotations of the images from the XML files was not compatible with the colab processings. It might be that the images are png and have 4 channels instead of 3 that .jpg images have and those functions here are optimized for JPG format, or something could be wrong on my calculations of the coordinates. The more images we can feed in the fine tunning the more accurate our detection could be. If we feed 600 images in fine tuning the the accuracy of detection would be state of the art, but also the dataset of images needs to be standardised.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kNFJSK3NVZt",
        "outputId": "05935f6d-9f8c-4bd0-98b5-82676a196b3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.50546448, 0.1953125 , 0.61748634, 0.28125   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def read_content(xml_file: str):\n",
        "\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    height = int(root.find(\"size\")[0].text)\n",
        "    width = int(root.find(\"size\")[1].text)\n",
        "    channels = int(root.find(\"size\")[2].text)\n",
        "    #print(\"height, width: \", height, width)\n",
        "\n",
        "    list_with_all_boxes = []\n",
        "\n",
        "    for member in root.findall('object'):\n",
        "        class_name = member[0].text\n",
        "\n",
        "        if class_name == 'without_mask':\n",
        "          class_name=0\n",
        "        elif class_name == 'with_mask':\n",
        "          class_name=1\n",
        "\n",
        "        filename = root.find('filename').text\n",
        "\n",
        "        ymin, xmin, ymax, xmax = None, None, None, None\n",
        "\n",
        "        ymin = int(member.find(\"bndbox/ymin\").text)\n",
        "        xmin = int(member.find(\"bndbox/xmin\").text)\n",
        "        ymax = int(member.find(\"bndbox/ymax\").text)\n",
        "        xmax = int(member.find(\"bndbox/xmax\").text)\n",
        "\n",
        "        if class_name==1:\n",
        "         # print(xmin, ymin, xmax, ymax)\n",
        "          list_with_single_boxes = [xmin/width, ymin/height, xmax/width, ymax/height]\n",
        "\n",
        "          list_with_all_boxes.append(list_with_single_boxes)\n",
        "        \n",
        "    return np.array(list_with_all_boxes)\n",
        "\n",
        "read_content('/content/annotationsFaceMask/maksssksksss0.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2PMtNcGTl4Z",
        "outputId": "426bf175-a87d-4ad0-faea-af9d2a4f01bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "gt_boxes = []\n",
        "for i in range(0, 25):\n",
        "  image_path = os.path.join('/content/annotationsFaceMask', 'maksssksksss' + str(i) + '.xml')\n",
        "  gt_boxes.append(read_content(image_path))\n",
        "gt_boxes =np.array(gt_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#give all annotation a 100% detection score for plot visualization\n",
        "dummy_scores = np.array([1.0]*25, dtype=np.float32)  # give boxes a score of 100%\n",
        "\n",
        "plt.figure(figsize=(30, 15))\n",
        "#choose the 25 first annotated images\n",
        "for idx in range(25):\n",
        "  plt.subplot(6, 5, idx+1)\n",
        "  if gt_boxes[idx] is not None:\n",
        "    #run plot_detections above\n",
        "    plot_detections(\n",
        "        train_images_np[idx],\n",
        "        gt_boxes[idx],\n",
        "        np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "        dummy_scores, category_index)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cIglEDAvpDAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CrLmB5ZsyukP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MASK DETECTION based on TF Object Detection AIP.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}